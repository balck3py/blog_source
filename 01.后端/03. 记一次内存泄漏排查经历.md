---
title: .NET Core K8s 容器内存飙高排查指南
date: 2025-12-17
author: Eugen
tags:
  - OOM
  - 内存泄露
  - .NET Core
  - Kubernetes
  - 内存诊断
  - dotnet-counters
  - dotnet-dump
---
# .NET Core K8s 容器内存飙高排查指南

## 1. 问题背景
*   **现象**：Kubernetes Pod 内存占用增长后不会释放（例如 2GB+），但并没有发生 OOM（Out Of Memory）重启。
*   **环境**：.NET Core 7.0 运行在 Linux 容器中。
*   **目标**：区分是 C# 代码层面的内存泄漏（Managed Leak）还是底层非托管内存占用（Unmanaged/Native Leak）。

## 2. 诊断工具准备

由于容器基础镜像通常精简（如 Alpine），需要手动安装诊断工具。

### 2.1 安装 dotnet-counters
用于查看实时 GC、CPU 和内存指标。

**遇到的坑与解决：**
*   **报错**：提示工具版本（如 9.0）不支持当前的运行时（如 7.0）。
*   **解决方法**：强制指定版本安装。

```bash
# 进入 Pod 内部
kubectl exec -it <pod-name> -- bash

# 生产环境一般都是只有runtime没有sdk，工具需要sdk才可以安装
# 安装wget
apt update && apt install -y wget

# 下载.net sdk
wget https://builds.dotnet.microsoft.com/dotnet/Sdk/7.0.410/dotnet-sdk-7.0.410-linux-x64.tar.gz

# 解压到dotnet文件夹，但是由于一般生产环境的dotnet都是runtime指向，所以需要到$HOME/dotnet的绝对目录下使用./dotnet命令安装工具
mkdir -p $HOME/dotnet && tar zxf dotnet-sdk-7.0.410-linux-x64.tar.gz -C $HOME/dotnet

# 安装匹配版本的 counters 工具(不带--version默认是当前最新版本)
./dotnet tool install --global dotnet-counters --version 7.*

# 将工具路径加入环境变量（以便直接运行）
export PATH="$PATH:/root/.dotnet/tools"
```

## 3. 数据采集与分析

### 3.1 实时监控
在导出之前，可以先使用PID监控下当前RunTime下各部分内存各个指标的占用情况
```bash

# 先查找出System.Runtime进程的PID（如截图所示，PID为：1）
dotnet-counters ps

# 查看实时监控：dotnet-counters monitor -p <PID> --refresh-interval 1 --counters System.Runtime
# --refresh-interval代表刷新频率，单位为秒，如果一直没有刷新，或者始终为0，则代表
dotnet-counters monitor -p 1 --refresh-interval 1 --counters System.Runtime

```

![[../00.assets/Pasted image 20251212164204.png]]
![[../00.assets/Pasted image 20251212164003.png]]
1. **重点观察以下指标**：
    - GC Heap Size (MB): 如果这个值持续上涨且 GC（垃圾回收）后不下降，说明有托管内存泄漏。
    - Gen 2 GC Count: 如果二代 GC 频繁发生但内存降不下来，问题很大。
    - LOH Size: 大对象堆是否一直在涨？
    - Working Set (MB): 物理内存占用。如果 Working Set 涨但 GC Heap 不涨，可能是**非托管内存泄漏**（Native Leak）。
### 3.2 采集步骤
不要只看实时监控，建议生成 CSV 文件以便分析趋势。

```bash
# -p 1: 监控 PID 1 (通常是入口进程)
# --refresh-interval 1: 每秒采集一次
# --counters System.Runtime: 必须指定，否则可能采不到数据
# 有时候命令安装完后，环境变量可能没添加成功，也需要到绝对目录下/root/.dotnet/tools使用指定文件的方式运行: ./dotnet-counters collect
dotnet-counters collect -p 1 --format csv --output /tmp/result.csv --refresh-interval 1 --counters System.Runtime

# 也可以使用docker cp或者kubectl cp命令将文件拿下来在本地分析
kubectl cp <container-detail-name>:/tmp/result.csv <local-path>:/result.csv -n <namespace-name>
```
*注意：运行命令后，会记录变动，但是不会捕获历史的变动，目的是收集截止到目前每一次变动时托管和非托管内存的占比，需保持运行状态至少 30-60 秒，期间可模拟访问服务，采集完成后按 `Q` 退出。*

### 3.3 关键指标解读 (核心逻辑)

打开生成的 `result.csv`，重点对比以下两个指标：

| 指标名称 | 含义 |
| :--- | :--- |
| **Working Set (MB)** | **物理内存占用总量** (Task Manager/K8s 看到的内存)。包含托管堆 + 非托管内存 + 共享库。 |
| **GC Heap Size (MB)** | **托管堆大小** (C# 代码创建的对象)。 |

#### 判定标准：

*   **场景 A：C# 代码内存泄漏 (Managed Leak)**
    *   **特征**：`GC Heap Size` 随着 `Working Set` 一起持续上涨，且 GC 无法将其回收（Gen 2 持续变大）。
    *   **对策**：使用 `dotnet-dump` 抓取快照，用 VS 分析引用链。

*   **场景 B：非托管内存占用 (Native Memory / Unmanaged)**
    *   **特征**：`Working Set` 很高（如 2GB），但 `GC Heap Size` 很低（如 50MB）。
    *   **结论**：问题不在 C# 对象，而在底层 C++ 库、Glibc 内存分配器或图形/压缩组件。

### 3.4 更进一步，使用快照（Dump）
这一步主要是为了进一步定位内存的泄露点，具体是托管还是非托管泄露，以及泄露的位置，dump的文件可以使用visual studio打开分析  
#### 3.4.1 使用 dotnet-gcdump (推荐首选)
**特点**：轻量、速度快、只包含托管堆的对象引用关系，**不会暂停程序太久**，生成的 .gcdump 文件很小，可以直接用 Visual Studio 打开。  
	  建议操作：在内存正常时抓一个，内存飙升后抓一个，方便对比。
```bash

# 安装dotnet-gcdump工具
./dotnet tool install --global dotnet-gcdump --version 7.*

# 生成快照dotnet-gcdump collect -p <PID>
dotnet-gcdump collect -p 1

```
**分析**：  
    将生成的 .gcdump 文件拖入 Visual Studio，使用 **"Compare to" (对比)** 功能，对比两份快照，立刻就能看到哪种类型的对象数量激增。

#### 3.4.2 使用 dotnet-dump (重型武器)
**特点**：全量 Dump，包含所有内存信息（代码、线程、托管和非托管堆）。**生成时会暂停应用程序**（STW），文件非常大。
```bash

# 安装dotnet-dump工具
./dotnet tool install --global dotnet-dump --version 7.*

# 生成完整快照：dotnet-dump collect -p <PID>
dotnet-dump collect -p 1

```
**分析方式1：使用 Visual Studio (Windows)**
如果开发机是 Windows，直接把 Linux 生成的 dump 文件下载下来（注意文件可能几 GB），拖进 Visual Studio。
- 点击 "Debug Managed Memory"。
    查看 "Paths to Root" 找出引用链。

## 4. 案例实战分析 (基于本次数据)

### 4.1 数据表现
*   **Working Set**: 稳定在 **2095 MB**，几乎无波动。
*   **GC Heap Size**: 在 **27 MB ~ 50 MB** 之间呈现锯齿状波动。
*   **Exception Count**: 每分钟固定时间（第45秒）抛出 5 个异常。

### 4.2 分析结论
1.  **GC 非常健康**：锯齿状图形说明 GC 正在正常工作，能够回收临时对象，没有托管内存泄漏。
2.  **巨大的非托管缺口**：
    > 2095MB (总内存) - 50MB (托管内存) ≈ **2045MB (非托管内存)**
    这表明 98% 的内存被非 .NET Runtime 管理的区域占用。

### 4.3 常见嫌疑对象 (Unmanaged Memory)
1.  **图形处理**：`System.Drawing.Common` (libgdiplus), `SkiaSharp`, `ImageSharp` (处理大图未 Dispose)。
2.  **Linux 内存分配器碎片**：Linux 的 `malloc` 在多线程高并发下，为了性能会申请多个 Arena (内存池) 并不归还给操作系统。
3.  **压缩/解压**：`GZipStream`, `DeflateStream` 等流操作。
4.  **网络/IO 缓冲**：gRPC 或 HttpClient 创建了过多的连接导致底层 Socket 缓冲区堆积。

## 5. 解决方案与下一步行动

### 5.1 优先尝试：调整 MALLOC 环境变量 (成本最低)
针对 Linux 容器内存碎片问题，这是最有效的“魔法”配置。

在 `Dockerfile` 或 K8s `Deployment.yaml` 的环境变量中添加：
```yaml
env:
  - name: MALLOC_ARENA_MAX
    value: "2"
```
*原理：限制 Glibc 分配的内存池数量，强制其更积极地复用内存，防止物理内存虚高。*

### 5.2 代码排查方向
如果不生效，需排查代码中是否使用了非托管资源：
*   **搜索关键字**：`Bitmap`, `Image`, `GZip`, `IntPtr`, `Marshal`。
*   **检查 IDisposable**：确保所有涉及非托管资源的对象都包裹在 `using (...) {}` 块中。

### 5.3 关注异常日志
日志中发现每分钟第 45 秒有 5 个异常抛出。
*   **行动**：检查应用日志（Log），定位这 5 个异常的来源。
*   **关联性**：如果是某种定时任务失败（如连接数据库失败、加载配置失败），可能会导致底层重试逻辑不断申请 Buffer 且未释放。

### 5.4 进阶分析 (如果以上都无效)
查看 Linux 进程的内存映射，确认是大块匿名内存 (anon) 还是某个特定 so 库占用。

```bash
# 在 Pod 内部运行
pmap -x 1 | sort -rn -k3 | head -20
```

---

## 6. 常用命令速查表

| 操作                | 命令                                                              |
| :---------------- | :-------------------------------------------------------------- |
| **安装工具 (指定版本)**   | `dotnet tool install -g dotnet-counters --version 7.*`          |
| **实时监控**          | `dotnet-counters monitor -p 1`                                  |
| **收集 CSV 数据**     | `dotnet-counters collect -p 1 --format csv --output result.csv` |
| **抓取内存快照 (Dump)** | `dotnet-dump collect -p 1`                                      |
| **分析 Dump (命令行)** | `dotnet-dump analyze mem.dmp`                                   |
| **查看系统内存详情**      | `cat /proc/meminfo` 或 `cat /sys/fs/cgroup/memory/memory.stat`   |
